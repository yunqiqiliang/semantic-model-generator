import os
import re
from collections import defaultdict
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional

from clickzetta.zettapark.session import Session
from loguru import logger

from semantic_model_generator.data_processing import data_types, proto_utils
from semantic_model_generator.protos import semantic_model_pb2
from semantic_model_generator.clickzetta_utils.clickzetta_connector import (
    AUTOGEN_TOKEN,
    DIMENSION_DATATYPES,
    MEASURE_DATATYPES,
    OBJECT_DATATYPES,
    TIME_MEASURE_DATATYPES,
    get_table_representation,
    get_valid_schemas_tables_columns_df,
)
from semantic_model_generator.clickzetta_utils.utils import create_fqn_table
from semantic_model_generator.validate.context_length import validate_context_length
from semantic_model_generator.llm import (
    DashscopeClient,
    DashscopeSettings,
    enrich_semantic_model,
    get_dashscope_settings,
)
from semantic_model_generator.validate.keywords import CZ_RESERVED_WORDS

_PLACEHOLDER_COMMENT = "  "
_FILL_OUT_TOKEN = " # <FILL-OUT>"
# TODO add _AUTO_GEN_TOKEN to the end of the auto generated descriptions.
_AUTOGEN_COMMENT_TOKEN = (
    " # <AUTO-GENERATED DESCRIPTION, PLEASE MODIFY AND REMOVE THE __ AT THE END>"
)
_DEFAULT_N_SAMPLE_VALUES_PER_COL = 3
_AUTOGEN_COMMENT_WARNING = f"# NOTE: This file was auto-generated by the semantic model generator. Please fill out placeholders marked with {_FILL_OUT_TOKEN} (or remove if not relevant) and verify autogenerated comments.\n"

def _singularize(token: str) -> str:
    if token.endswith("IES") and len(token) > 3:
        return token[:-3] + "Y"
    if token.endswith(("SES", "XES", "ZES", "CHES", "SHES")) and len(token) > 4:
        return token[:-2]
    if token.endswith("S") and len(token) > 3:
        return token[:-1]
    return token


def _clean_column_type(raw: Any) -> str:
    if raw is None:
        return ""
    text = str(raw).strip()
    if not text:
        return ""
    text = re.sub(r"\s+", " ", text.upper())
    for suffix in (" NOT NULL", " NULL"):
        if text.endswith(suffix):
            text = text[: -len(suffix)].strip()
    return text


def _base_type_from_type(column_type: str) -> str:
    cleaned = _clean_column_type(column_type)
    token = cleaned.split(" ")[0] if cleaned else ""
    return token.split("(")[0]


def _identifier_tokens(name: str, prefixes_to_drop: Optional[set[str]] = None) -> List[str]:
    name = name.replace("-", "_")
    raw_tokens = re.split(r"[^0-9A-Za-z]+", name)
    tokens: List[str] = []
    for token in raw_tokens:
        if not token:
            continue
        split = re.sub(r"([a-z0-9])([A-Z])", r"\1 \2", token).split()
        for part in split:
            tokens.append(part.upper())
    tokens = [token for token in tokens if token]
    if prefixes_to_drop and len(tokens) >= 2 and tokens[0] in prefixes_to_drop:
        tokens = tokens[1:]
    return tokens


def _sanitize_identifier_name(name: str, prefixes_to_drop: Optional[set[str]] = None) -> str:
    if not name:
        return ""

    cleaned = name.replace("-", "_")
    cleaned = re.sub(r"\s+", "_", cleaned)
    parts = [part for part in cleaned.split("_") if part]
    if not parts:
        return ""

    if len(parts) >= 2 and len(parts[0]) == 1:
        parts = parts[1:]

    if prefixes_to_drop:
        drop_tokens = {token.upper() for token in prefixes_to_drop}
        while parts and parts[0].upper() in drop_tokens:
            parts = parts[1:]

    if not parts:
        return ""

    rebuilt = "_".join(parts)
    tokens = _identifier_tokens(rebuilt)
    if not tokens:
        return ""

    if len(parts) == 1:
        return "".join(tokens)
    return "_".join(tokens)


_INVALID_ID_CHARS = re.compile(r"[^0-9A-Za-z_$]")


def _is_valid_identifier(name: str) -> bool:
    if not name:
        return False
    candidate = name.strip()
    if not candidate:
        return False
    return candidate.replace("_", "").replace("$", "").isalnum()


def _normalize_identifier(name: str) -> str:
    if not name:
        return ""
    text = re.sub(r"\s+", "_", name.strip())
    text = text.replace("-", "_")
    text = _INVALID_ID_CHARS.sub("", text)
    text = re.sub(r"_+", "_", text)
    return text.strip("_")


def _safe_semantic_identifier(
    raw_name: str,
    used_names: set[str],
    fallback_prefix: str,
    prefixes_to_drop: Optional[set[str]] = None,
) -> str:
    """
    Produce a YAML-safe identifier that avoids reserved keywords and duplicates.
    """

    variants: List[str] = []
    normalized = _normalize_identifier(raw_name)
    if normalized:
        variants.append(normalized)
    sanitized = _sanitize_identifier_name(raw_name, prefixes_to_drop=prefixes_to_drop)
    if sanitized:
        variants.append(sanitized.lower())
    variants.append(f"{fallback_prefix}_field")

    for variant in variants:
        candidate = _normalize_identifier(variant)
        if not candidate:
            continue
        if candidate[0].isdigit():
            candidate = f"{fallback_prefix}_{candidate}"
        candidate = candidate.lower()
        base = candidate
        suffix = 2
        while (
            base in used_names
            or base.upper() in CZ_RESERVED_WORDS
            or not _is_valid_identifier(base)
        ):
            base = f"{candidate}_{suffix}"
            suffix += 1
        if base.upper() in CZ_RESERVED_WORDS or not _is_valid_identifier(base):
            continue
        used_names.add(base)
        return base

    suffix = 1
    while True:
        candidate = f"{fallback_prefix}_{suffix}"
        if (
            candidate not in used_names
            and candidate.upper() not in CZ_RESERVED_WORDS
            and _is_valid_identifier(candidate)
        ):
            used_names.add(candidate)
            return candidate
        suffix += 1


def _is_identifier_like(column_name: str, base_type: str) -> bool:
    """
    Heuristic to detect identifier-style numeric columns that should stay as dimensions.
    """

    tokens = _identifier_tokens(column_name)
    if not tokens:
        return False

    tail = tokens[-1]
    if tail in {"ID", "KEY", "ROWID", "ROW_ID", "PK", "PRIMARY", "PRIMARYKEY"}:
        return True
    if tail.endswith("ID") and len(tail) > 2:
        return True
    if tail.endswith("KEY") and len(tail) > 3:
        return True

    return False


def _table_variants(table_name: str) -> set[str]:
    table_upper = table_name.upper()
    tokens = _identifier_tokens(table_name)
    variants = {table_upper}
    variants.update(tokens)
    for token in list(variants):
        variants.add(_singularize(token))
        if len(token) > 3:
            variants.add(token[:4])
        if len(token) > 3:
            variants.add(token[:3])
        if len(token) > 3:
            variants.add(token[-4:])
        if len(token) > 2:
            variants.add(token[-3:])
    return {variant for variant in variants if variant}


_GENERIC_PREFIXES = {
    "DIM",
    "FACT",
    "FCT",
    "BRIDGE",
    "BRG",
    "STG",
    "ODS",
    "OD",
    "DW",
    "VW",
    "VIEW",
    "HUB",
    "SAT",
    "LNK",
    "TMP",
    "TMPV",
}


def _table_prefixes(table_name: str) -> set[str]:
    prefixes: set[str] = set()
    tokens = _identifier_tokens(table_name)
    if not tokens:
        tokens = [table_name.upper()]

    first_token = tokens[0].upper()
    if first_token in _GENERIC_PREFIXES:
        for length in range(1, min(len(first_token), 4) + 1):
            prefixes.add(first_token[:length])
        prefixes.add(first_token)
    return prefixes


def _looks_like_primary_key(table_name: str, column_name: str) -> bool:
    upper_name = column_name.strip().upper()
    variants = _table_variants(table_name)
    direct_matches = {
        "ID",
        "PK",
        "PRIMARY_KEY",
    }
    for variant in variants:
        direct_matches.update({f"{variant}_ID", f"{variant}ID", f"{variant}_KEY", f"{variant}KEY"})
    if upper_name in direct_matches:
        return True

    tokens = _identifier_tokens(column_name)
    if not tokens:
        return False
    last = tokens[-1]
    if last in {"ID", "KEY"} and len(tokens) >= 2 and tokens[-2] in variants:
        return True
    if last.endswith("ID") and last[:-2] in variants:
        return True
    if last.endswith("KEY") and last[:-3] in variants:
        return True
    if len(tokens) == 1 and last.endswith(("ID", "KEY")):
        stem = last[:-2] if last.endswith("ID") else last[:-3]
        if stem in variants:
            return True

    return False


_TIME_NAME_HINTS = ("DATE", "TIME", "TIMESTAMP", "DT", "DAY", "HOUR")
_ISO_DATE_REGEX = re.compile(r"^\d{4}[-/]\d{2}[-/]\d{2}")


def _is_time_like_column(column: data_types.Column) -> bool:
    base_type = _base_type_from_type(column.column_type)
    if base_type in TIME_MEASURE_DATATYPES:
        return True
    if base_type not in {"STRING", "VARCHAR", "TEXT", "CHAR", "CHARACTER", "NVARCHAR"}:
        return False

    column_name = column.column_name.upper()
    if any(hint in column_name for hint in _TIME_NAME_HINTS):
        return True

    for value in (column.values or [])[:5]:
        val = str(value).strip()
        if not val:
            continue
        candidate = val
        if candidate.endswith("Z"):
            candidate = candidate[:-1]
        candidate = candidate.replace(" ", "T", 1)
        try:
            datetime.fromisoformat(candidate)
            return True
        except ValueError:
            pass
        if _ISO_DATE_REGEX.match(val):
            return True
    return False


def _format_literal(value: str, base_type: str) -> str:
    text = "" if value is None else str(value)
    if base_type in MEASURE_DATATYPES:
        try:
            if "." in text:
                return str(float(text))
            return str(int(text))
        except ValueError:
            pass
    lowered = text.lower()
    if lowered in {"true", "false"}:
        return lowered.upper()
    escaped = text.replace("'", "''")
    return f"'{escaped}'"


def _suggest_filters(raw_table: data_types.Table) -> List[semantic_model_pb2.NamedFilter]:
    suggestions: List[semantic_model_pb2.NamedFilter] = []
    for col in raw_table.columns:
        base_type = _base_type_from_type(col.column_type)
        values = col.values or []
        distinct_values: List[str] = []
        for value in values:
            if value not in distinct_values:
                distinct_values.append(value)
        if _is_time_like_column(col):
            expr = f"{col.column_name} >= DATEADD('day', -30, CURRENT_DATE())"
            suggestions.append(
                semantic_model_pb2.NamedFilter(
                    name=f"{col.column_name}_last_30_days",
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=_PLACEHOLDER_COMMENT,
                    expr=expr,
                )
            )
        if 1 < len(distinct_values) <= 5:
            upper_name = col.column_name.upper()
            is_identifier_like = upper_name.endswith(("ID", "_ID", "KEY", "_KEY"))

            categorical_suffixes = (
                "STATUS",
                "FLAG",
                "TYPE",
                "PRIORITY",
                "SEGMENT",
                "CATEGORY",
                "MODE",
                "CODE",
                "LEVEL",
            )
            is_textual = base_type in {"STRING", "TEXT", "VARCHAR", "CHAR", "CHARACTER"}
            is_boolean = base_type in {"BOOLEAN"}
            is_categorical_numeric = base_type in {"INT", "INTEGER", "NUMBER", "SMALLINT", "BIGINT"} and any(
                upper_name.endswith(suffix) for suffix in categorical_suffixes
            )

            if not is_identifier_like and (is_textual or is_boolean or is_categorical_numeric):
                formatted = [_format_literal(val, base_type) for val in distinct_values[:5]]
                expr = f"{col.column_name} IN ({', '.join(formatted)})"
                suggestions.append(
                    semantic_model_pb2.NamedFilter(
                        name=f"{col.column_name}_include_values",
                        synonyms=[_PLACEHOLDER_COMMENT],
                        description=_PLACEHOLDER_COMMENT,
                        expr=expr,
                    )
                )
    return suggestions


def _infer_relationships(
    raw_tables: List[tuple[data_types.FQNParts, data_types.Table]]
) -> List[semantic_model_pb2.Relationship]:
    relationships: List[semantic_model_pb2.Relationship] = []
    if not raw_tables:
        return relationships

    metadata = {}
    prefix_counter: Dict[str, int] = {}
    for _, raw_table in raw_tables:
        for column in raw_table.columns:
            tokens = _identifier_tokens(column.column_name)
            if tokens:
                prefix = tokens[0]
                prefix_counter[prefix] = prefix_counter.get(prefix, 0) + 1
    global_prefixes = {
        prefix
        for prefix, count in prefix_counter.items()
        if len(prefix) <= 3 and count >= 2
    }

    metadata = {}
    for fqn, raw_table in raw_tables:
        columns_meta: Dict[str, Dict[str, Any]] = {}
        pk_candidates: Dict[str, List[str]] = defaultdict(list)
        table_prefixes = global_prefixes | _table_prefixes(raw_table.name)
        for column in raw_table.columns:
            base_type = _base_type_from_type(column.column_type)
            normalized = _sanitize_identifier_name(column.column_name, prefixes_to_drop=table_prefixes)
            entry = columns_meta.setdefault(
                normalized,
                {
                    "names": [],
                    "base_type": base_type,
                    "values": [],
                    "is_identifier": False,
                    "is_primary": False,
                },
            )
            entry["base_type"] = base_type
            entry["names"].append(column.column_name)
            if column.values:
                entry["values"].extend(column.values)
            entry["is_identifier"] = entry["is_identifier"] or _is_identifier_like(column.column_name, base_type)
            is_primary = getattr(column, "is_primary_key", False)
            if is_primary:
                entry["is_primary"] = True
                if column.column_name not in pk_candidates[normalized]:
                    pk_candidates[normalized].append(column.column_name)
                continue
            if _looks_like_primary_key(raw_table.name, column.column_name):
                pk_candidates[normalized].append(column.column_name)
        metadata[raw_table.name] = {
            "fqn": fqn,
            "columns": columns_meta,
            "pk_candidates": pk_candidates,
        }

    pairs: dict[tuple[str, str], List[tuple[str, str]]] = {}

    def _record_pair(left_table: str, right_table: str, left_col: str, right_col: str) -> None:
        key = (left_table, right_table)
        value = (left_col, right_col)
        if value not in pairs.setdefault(key, []):
            pairs[key].append(value)

    table_names = list(metadata.keys())
    for i in range(len(table_names)):
        for j in range(i + 1, len(table_names)):
            table_a_name = table_names[i]
            table_b_name = table_names[j]
            table_a = metadata[table_a_name]
            table_b = metadata[table_b_name]

            shared = set(table_a["columns"].keys()) & set(table_b["columns"].keys())
            for col_key in shared:
                meta_a = table_a["columns"][col_key]
                meta_b = table_b["columns"][col_key]
                if meta_a["base_type"] != meta_b["base_type"]:
                    continue
                in_pk_a = col_key in table_a["pk_candidates"]
                in_pk_b = col_key in table_b["pk_candidates"]
                if in_pk_a and not in_pk_b:
                    _record_pair(
                        table_b_name,
                        table_a_name,
                        meta_b["names"][0],
                        meta_a["names"][0],
                    )
                elif in_pk_b and not in_pk_a:
                    _record_pair(
                        table_a_name,
                        table_b_name,
                        meta_a["names"][0],
                        meta_b["names"][0],
                    )
                elif in_pk_a and in_pk_b:
                    pk_count_a = len(table_a["pk_candidates"])
                    pk_count_b = len(table_b["pk_candidates"])
                    if pk_count_a >= 2 and pk_count_b == 1:
                        _record_pair(
                            table_a_name,
                            table_b_name,
                            meta_a["names"][0],
                            meta_b["names"][0],
                        )
                    elif pk_count_b >= 2 and pk_count_a == 1:
                        _record_pair(
                            table_b_name,
                            table_a_name,
                            meta_b["names"][0],
                            meta_a["names"][0],
                        )

            # Suffix-based matches (e.g. order_date_id -> date_id)
            for pk_norm, pk_cols in table_a["pk_candidates"].items():
                pk_meta = table_a["columns"].get(pk_norm)
                if not pk_meta:
                    continue
                for norm_b, meta_b in table_b["columns"].items():
                    if meta_b["base_type"] != pk_meta["base_type"]:
                        continue
                    if norm_b == pk_norm:
                        continue
                    if norm_b.endswith(pk_norm):
                        _record_pair(
                            table_b_name,
                            table_a_name,
                            meta_b["names"][0],
                            pk_cols[0],
                        )

            for pk_norm, pk_cols in table_b["pk_candidates"].items():
                pk_meta = table_b["columns"].get(pk_norm)
                if not pk_meta:
                    continue
                for norm_a, meta_a in table_a["columns"].items():
                    if meta_a["base_type"] != pk_meta["base_type"]:
                        continue
                    if norm_a == pk_norm:
                        continue
                    if norm_a.endswith(pk_norm):
                        _record_pair(
                            table_a_name,
                            table_b_name,
                            meta_a["names"][0],
                            pk_cols[0],
                        )

    for (left_table, right_table), column_pairs in pairs.items():
        relationship = semantic_model_pb2.Relationship(
            name=f"{left_table}_to_{right_table}",
            left_table=left_table,
            right_table=right_table,
            join_type=semantic_model_pb2.JoinType.inner,
            relationship_type=semantic_model_pb2.RelationshipType.many_to_one,
        )
        for left_column, right_column in column_pairs:
            relationship.relationship_columns.append(
                semantic_model_pb2.RelationKey(
                    left_column=left_column, right_column=right_column
                )
            )
        relationships.append(relationship)
    return relationships


def _raw_table_to_semantic_context_table(
    database: str, schema: str, raw_table: data_types.Table
) -> semantic_model_pb2.Table:
    """
    Converts a raw table representation to a semantic model table in protobuf format.

    Args:
        database (str): The name of the database containing the table.
        schema (str): The name of the schema containing the table.
        raw_table (data_types.Table): The raw table object to be transformed.

    Returns:
        semantic_model_pb2.Table: A protobuf representation of the semantic table.

    This function categorizes table columns into TimeDimensions, Dimensions, or Measures based on their data type,
    populates them with sample values, and sets placeholders for descriptions and filters.
    """

    # For each column, decide if it is a TimeDimension, Measure, or Dimension column.
    # For now, we decide this based on datatype.
    # Any time datatype, is TimeDimension.
    # Any varchar/text is Dimension.
    # Any numerical column is Measure.

    time_dimensions = []
    dimensions = []
    facts: List[semantic_model_pb2.Fact] = []
    used_identifier_names: set[str] = set()
    table_prefixes = _table_prefixes(raw_table.name)

    for col in raw_table.columns:
        base_type = _base_type_from_type(col.column_type)
        if _is_time_like_column(col):
            time_data_type = col.column_type
            if time_data_type.split("(")[0].upper() in {"STRING", "VARCHAR", "TEXT", "CHAR", "CHARACTER", "NVARCHAR"}:
                time_data_type = "TIMESTAMP_NTZ"
            time_dimension_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "time_dimension",
                prefixes_to_drop=table_prefixes,
            )
            time_dimensions.append(
                semantic_model_pb2.TimeDimension(
                    name=time_dimension_name,
                    expr=col.column_name,
                    data_type=time_data_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )

        elif base_type in DIMENSION_DATATYPES:
            dimension_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "dimension",
                prefixes_to_drop=table_prefixes,
            )
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=dimension_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )

        elif base_type in MEASURE_DATATYPES:
            if _is_identifier_like(col.column_name, base_type):
                identifier_dimension_name = _safe_semantic_identifier(
                    col.column_name,
                    used_identifier_names,
                    "dimension",
                    prefixes_to_drop=table_prefixes,
                )
                dimensions.append(
                    semantic_model_pb2.Dimension(
                        name=identifier_dimension_name,
                        expr=col.column_name,
                        data_type=col.column_type,
                        sample_values=col.values,
                        synonyms=[_PLACEHOLDER_COMMENT],
                        description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                    )
                )
                continue
            fact_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "fact",
                prefixes_to_drop=table_prefixes,
            )
            facts.append(
                semantic_model_pb2.Fact(
                    name=fact_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )
        elif base_type in OBJECT_DATATYPES:
            logger.warning(
                f"""We don't currently support {col.column_type} as an input column datatype to the Semantic Model. We are skipping column {col.column_name} for now."""
            )
            continue
        else:
            fallback_dimension_name = _safe_semantic_identifier(
                col.column_name,
                used_identifier_names,
                "dimension",
                prefixes_to_drop=table_prefixes,
            )
            logger.warning(
                f"Column datatype does not map to a known datatype. Input was = {col.column_type}. We are going to place as a Dimension for now."
            )
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=fallback_dimension_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )
    if len(time_dimensions) + len(dimensions) + len(facts) == 0:
        raise ValueError(
            f"No valid columns found for table {raw_table.name}. Please verify that this table contains column's datatypes not in {OBJECT_DATATYPES}."
        )

    filters = _suggest_filters(raw_table)

    return semantic_model_pb2.Table(
        name=raw_table.name,
        base_table=semantic_model_pb2.FullyQualifiedTable(
            database=database, schema=schema, table=raw_table.name
        ),
        # For fields we can not automatically infer, leave a comment for the user to fill out.
        description=raw_table.comment if raw_table.comment else _PLACEHOLDER_COMMENT,
        filters=filters,
        dimensions=dimensions,
        time_dimensions=time_dimensions,
        facts=facts,
    )


def raw_schema_to_semantic_context(
    base_tables: List[str],
    semantic_model_name: str,
    conn: Session,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = True,
    enrich_with_llm: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None,
) -> semantic_model_pb2.SemanticModel:
    """
    Converts a list of fully qualified ClickZetta table names into a semantic model.

    Parameters:
    - base_tables  (list[str]): Fully qualified table names to include in the semantic model.
    - semantic_model_name (str): A meaningful semantic model name.
    - conn (Session): ClickZetta session to reuse.
    - n_sample_values (int): The number of sample values per col.

    Returns:
    - The semantic model (semantic_model_pb2.SemanticModel).

    This function fetches metadata for the specified tables, performs schema validation, extracts key information,
    enriches metadata from ClickZetta, and constructs a semantic model in protobuf format.
    It handles different workspaces and schemas within the same account by reusing the provided session.

    Raises:
    - AssertionError: If no valid tables are found in the specified schema.
    """

    # For FQN tables, the connector handles cross-schema access automatically.
    def _notify(message: str) -> None:
        if progress_callback:
            try:
                progress_callback(message)
            except Exception:
                logger.debug("Progress callback failed for message: {}", message)

    table_objects = []
    raw_tables_metadata: List[tuple[data_types.FQNParts, data_types.Table]] = []
    unique_database_schema: List[str] = []
    for table in base_tables:
        # Verify this is a valid FQN table. For now, we check that the table follows the following format.
        # {database}.{schema}.{table}
        fqn_table = create_fqn_table(table)
        fqn_databse_schema = f"{fqn_table.database}.{fqn_table.schema_name}"

        if fqn_databse_schema not in unique_database_schema:
            unique_database_schema.append(fqn_databse_schema)

        logger.info(f"Pulling column information from {fqn_table}")
        _notify(f"Fetching metadata for {fqn_table.database}.{fqn_table.schema_name}.{fqn_table.table}...")
        valid_schemas_tables_columns_df = get_valid_schemas_tables_columns_df(
            session=conn,
            workspace=fqn_table.database,
            table_schema=fqn_table.schema_name,
            table_names=[fqn_table.table],
        )
        if valid_schemas_tables_columns_df.empty:
            raise ValueError(
                (
                    "Unable to retrieve column metadata for table "
                    f"{fqn_table.database}.{fqn_table.schema_name}.{fqn_table.table}. "
                    "Shared or external catalogs (category=SHARED/EXTERNAL) do not expose information_schema views; "
                    "please copy the data into a managed workspace or provide manual metadata."
                )
            )

        # get the valid columns for this table.
        valid_columns_df_this_table = valid_schemas_tables_columns_df[
            valid_schemas_tables_columns_df["TABLE_NAME"] == fqn_table.table
        ]

        raw_table = get_table_representation(
            session=conn,
            workspace=fqn_table.database,
            schema_name=fqn_table.schema_name,
            table_name=fqn_table.table,  # Non-qualified table name
            table_index=0,
            ndv_per_column=n_sample_values,  # number of sample values to pull per column.
            columns_df=valid_columns_df_this_table,
            max_workers=1,
        )
        table_object = _raw_table_to_semantic_context_table(
            database=fqn_table.database,
            schema=fqn_table.schema_name,
            raw_table=raw_table,
        )
        table_objects.append(table_object)
        raw_tables_metadata.append((fqn_table, raw_table))
    # TODO(jhilgart): Call cortex model to generate a semantically friendly name here.

    relationships: List[semantic_model_pb2.Relationship] = []
    if allow_joins:
        relationships = _infer_relationships(raw_tables_metadata)

    context = semantic_model_pb2.SemanticModel(
        name=semantic_model_name,
        tables=table_objects,
        relationships=relationships,
    )
    context.description = _PLACEHOLDER_COMMENT
    context.custom_instructions = _PLACEHOLDER_COMMENT

    if enrich_with_llm:
        settings = get_dashscope_settings()
        if settings:
            actual_model = "qwen-plus-latest"
            logger.info(
                "Running DashScope enrichment for semantic model '{}' using model '{}'",
                semantic_model_name,
                actual_model,
            )
            _notify("Running DashScope enrichment to enhance descriptions and metrics...")
            settings = DashscopeSettings(
                api_key=settings.api_key,
                model=actual_model,
                base_url=settings.base_url,
                temperature=settings.temperature,
                top_p=settings.top_p,
                max_output_tokens=settings.max_output_tokens,
                timeout_seconds=settings.timeout_seconds,
            )
            client = DashscopeClient(settings)
            enrich_semantic_model(
                context,
                raw_tables_metadata,
                client,
                placeholder=_PLACEHOLDER_COMMENT,
            )
            _notify("DashScope enrichment complete.")
        else:
            logger.warning("LLM enrichment was requested but DashScope is not configured; skipping enrichment.")
            _notify("DashScope configuration missing; skipped enrichment.")
    return context


def append_comment_to_placeholders(yaml_str: str) -> str:
    """
    Finds all instances of a specified placeholder in a YAML string and appends a given text to these placeholders.
    This is the homework to fill out after your yaml is generated.

    Parameters:
    - yaml_str (str): The YAML string to process.

    Returns:
    - str: The modified YAML string with appended text to placeholders.
    """
    updated_yaml = []
    # Split the string into lines to process each line individually
    lines = yaml_str.split("\n")

    for line in lines:
        # Check if the placeholder is in the current line.
        # Strip the last quote to match.
        if line.rstrip("'").endswith(_PLACEHOLDER_COMMENT):
            # Replace the _PLACEHOLDER_COMMENT with itself plus the append_text
            updated_line = line + _FILL_OUT_TOKEN
            updated_yaml.append(updated_line)
        elif line.rstrip("'").endswith(AUTOGEN_TOKEN):
            updated_line = line + _AUTOGEN_COMMENT_TOKEN
            updated_yaml.append(updated_line)
        # Add comments to specific fields in certain sections.
        elif line.lstrip().startswith("join_type"):
            updated_yaml.append(line)
        elif line.lstrip().startswith("relationship_type"):
            updated_yaml.append(line)
        else:
            updated_yaml.append(line)

    # Join the lines back together into a single string
    return "\n".join(updated_yaml)


def _to_snake_case(s: str) -> str:
    """
    Convert a string into snake case.

    Parameters:
    s (str): The string to convert.

    Returns:
    str: The snake case version of the string.
    """
    # Replace common delimiters with spaces
    s = s.replace("-", " ").replace("_", " ")

    words = s.split(" ")

    # Convert each word to lowercase and join with underscores
    snake_case_str = "_".join([word.lower() for word in words if word]).strip()

    return snake_case_str


def generate_base_semantic_model_from_clickzetta(
    base_tables: List[str],
    conn: Session,
    semantic_model_name: str,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    output_yaml_path: Optional[str] = None,
) -> None:
    """
    Generates a base semantic context from specified ClickZetta tables and exports it to a YAML file.

    Parameters:
        base_tables : Fully qualified names of ClickZetta tables to include in the semantic context.
        conn: ClickZetta session to reuse.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        output_yaml_path: Path for the output YAML file. If None, defaults to 'semantic_model_generator/output_models/YYYYMMDDHHMMSS_<semantic_model_name>.yaml'.
        n_sample_values: The number of sample values to populate for all columns.

    Returns:
        None. Writes the semantic context to a YAML file.
    """
    formatted_datetime = datetime.now().strftime("%Y%m%d%H%M%S")
    if not output_yaml_path:
        file_name = f"{formatted_datetime}_{_to_snake_case(semantic_model_name)}.yaml"
        if os.path.exists("semantic_model_generator/output_models"):
            write_path = f"semantic_model_generator/output_models/{file_name}"
        else:
            write_path = f"./{file_name}"
    else:  # Assume user gives correct path.
        write_path = output_yaml_path

    yaml_str = generate_model_str_from_clickzetta(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        conn=conn,
    )

    with open(write_path, "w") as f:
        # Clarify that the YAML was autogenerated and that placeholders should be filled out/deleted.
        f.write(_AUTOGEN_COMMENT_WARNING)
        f.write(yaml_str)

    logger.info(f"Semantic model saved to {write_path}")

    return None


def generate_model_str_from_clickzetta(
    base_tables: List[str],
    semantic_model_name: str,
    conn: Session,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = True,
    enrich_with_llm: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None,
) -> str:
    """
    Generates a base semantic context from specified ClickZetta tables and returns the raw string.

    Parameters:
        base_tables : Fully qualified names of ClickZetta tables to include in the semantic context.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        conn: ClickZetta session to reuse.
        n_sample_values: The number of sample values to populate for all columns.
        allow_joins: Whether to allow joins in the semantic context.
        progress_callback: Optional callable invoked with human-readable progress updates.

    Returns:
        str: The raw string of the semantic context.
    """
    def _notify(message: str) -> None:
        if progress_callback:
            try:
                progress_callback(message)
            except Exception:
                logger.debug("Progress callback failed for message: {}", message)

    table_list = ", ".join(base_tables)
    logger.info("Generating semantic model '{}' from tables: {}", semantic_model_name, table_list)
    _notify("Collecting metadata from ClickZetta tables...")

    context = raw_schema_to_semantic_context(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        allow_joins=allow_joins,
        enrich_with_llm=enrich_with_llm,
        conn=conn,
        progress_callback=_notify,
    )
    _notify("Constructing semantic model structure...")
    # Validate the generated yaml is within context limits.
    # We just throw a warning here to allow users to update.
    validate_context_length(context)
    _notify("Validating semantic model context length...")

    _notify("Converting semantic model to YAML...")
    yaml_str = proto_utils.proto_to_yaml(context)
    # Once we have the yaml, update to include to # <FILL-OUT> tokens.
    yaml_str = append_comment_to_placeholders(yaml_str)

    _notify("Semantic model generation complete.")
    logger.info("Semantic model '{}' generated successfully.", semantic_model_name)
    return yaml_str
